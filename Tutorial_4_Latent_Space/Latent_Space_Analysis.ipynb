{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33eebb23",
   "metadata": {},
   "source": [
    "# üß† Tutorial 4: Latent Space Analysis\n",
    "\n",
    "## Understanding and Visualizing Protein Embeddings\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "1. Extract and manipulate protein embeddings from pre-trained ESM2 models\n",
    "2. Reduce high-dimensional embeddings to 2D for visualization\n",
    "3. Quantify clustering quality using mutual information metrics\n",
    "4. Optimize dimensionality reduction hyperparameters automatically\n",
    "5. Analyze how features change across different layers of a transformer model\n",
    "6. Interpret latent space structure in relation to protein function\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe822ba",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Environment Setup\n",
    "\n",
    "### 1.1 Importing Libraries\n",
    "\n",
    "Now we'll import all necessary libraries for latent space analysis, including transformers for protein models, UMAP for dimensionality reduction, and Optuna for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c22cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORE PYTHON LIBRARIES\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import zipfile\n",
    "import logging\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "import warnings\n",
    "\n",
    "# ============================================================\n",
    "# WARNING SUPPRESSION\n",
    "# ============================================================\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', module='tqdm')\n",
    "os.environ[\"TQDM_DISABLE\"] = \"0\"\n",
    "\n",
    "# ============================================================\n",
    "# DATA HANDLING & PROCESSING\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ============================================================\n",
    "# MACHINE LEARNING & DEEP LEARNING\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ============================================================\n",
    "# SCIKIT-LEARN UTILITIES\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.metrics import mutual_info_score, adjusted_mutual_info_score, normalized_mutual_info_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ============================================================\n",
    "# DIMENSIONALITY REDUCTION & OPTIMIZATION\n",
    "# ============================================================\n",
    "import umap\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================\n",
    "# STATISTICAL & SCIENTIFIC COMPUTING\n",
    "# ============================================================\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# ============================================================\n",
    "# UTILITIES\n",
    "# ============================================================\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect working directory and set base path for data files\n",
    "# This handles two common execution scenarios:\n",
    "# 1. Running from the notebook's directory (Tutorial_4_Latent_Space/)\n",
    "# 2. Running from /workspace with repo at /workspace/tutorials\n",
    "\n",
    "cwd = Path.cwd()\n",
    "target_dir = Path(\"data\") / \"care\"\n",
    "\n",
    "# Define scenarios to check\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"notebook directory\",\n",
    "        \"check\": lambda: cwd.name == \"Tutorial_4_Latent_Space\",\n",
    "        \"base\": Path(\"..\")\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"/workspace\",\n",
    "        \"check\": lambda: cwd == Path(\"/workspace\"),\n",
    "        \"base\": Path(\"/workspace/tutorials\")\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"current directory\",\n",
    "        \"check\": lambda: True,\n",
    "        \"base\": cwd\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"fallback (parent directory)\",\n",
    "        \"check\": lambda: True,\n",
    "        \"base\": Path(\"..\")\n",
    "    }\n",
    "]\n",
    "\n",
    "# Find the first scenario where data directory exists, or use fallback\n",
    "BASE_DIR = None\n",
    "for scenario in scenarios:\n",
    "    if BASE_DIR is not None:\n",
    "        break\n",
    "    if not scenario[\"check\"]():\n",
    "        continue\n",
    "    \n",
    "    base_dir = scenario[\"base\"]\n",
    "    data_dir = base_dir / target_dir\n",
    "    if data_dir.exists():\n",
    "        BASE_DIR = base_dir\n",
    "        scenario_name = scenario[\"name\"]\n",
    "        print(f\"‚úì Detected: Running from {scenario_name}\")\n",
    "        print(f\"  Current directory: {cwd}\")\n",
    "        print(f\"  Base directory: {BASE_DIR.resolve()}\")\n",
    "        break\n",
    "\n",
    "# If no scenario found data, use the fallback (last scenario)\n",
    "if BASE_DIR is None:\n",
    "    BASE_DIR = scenarios[-1][\"base\"]\n",
    "    print(f\"‚ö†Ô∏è  Warning: Unknown execution scenario, using fallback\")\n",
    "    print(f\"  Current directory: {cwd}\")\n",
    "    print(f\"  Base directory: {BASE_DIR.resolve()}\")\n",
    "\n",
    "print(f\"\\nüìÇ Using base directory: {BASE_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93f858",
   "metadata": {},
   "source": [
    "## üì• Step 2: Data Acquisition\n",
    "\n",
    "### 2.1 About the CARE Dataset\n",
    "\n",
    "We'll use the [**CARE** benchmarking dataset](https://github.com/jsunn-y/CARE/) containing protein sequences annotated with EC (Enzyme Commission) numbers.\n",
    "\n",
    "**Dataset Overview:**\n",
    "- Proteins classified by enzymatic function (EC numbers)\n",
    "- Multiple test splits for different similarity ranges\n",
    "- Ideal for analyzing how models represent functional relationships\n",
    "\n",
    "### 2.2 Feature Extraction Pipeline\n",
    "\n",
    "Our pipeline for extracting latent representations:\n",
    "1. **Download and prepare dataset** - Load CARE protein sequences\n",
    "2. **Select pre-trained model** - Use ESM2 with AutoTokenizer\n",
    "3. **Create encodings** - Tokenize sequences for model input\n",
    "4. **Forward pass** - Pass encodings through the model\n",
    "5. **Extract embeddings** - Obtain last hidden state (CLS token) as feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE DATA PATHS\n",
    "# ============================================================\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"care\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ZIP_PATH = DATA_DIR / \"datasets.zip\"\n",
    "ZENODO_URL = \"https://zenodo.org/record/12195378/files/datasets.zip?download=1\"\n",
    "\n",
    "print(\"‚úì Data directory configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD CARE DATASET FROM ZENODO\n",
    "# ============================================================\n",
    "if not ZIP_PATH.exists():\n",
    "    print(\"üì• Downloading CARE processed datasets from Zenodo...\")\n",
    "    print(\"‚è≥ This may take a minute...\")\n",
    "    \n",
    "    with requests.get(ZENODO_URL, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"content-length\", 0))\n",
    "        \n",
    "        with open(ZIP_PATH, \"wb\") as f, tqdm(total=total, unit='B', unit_scale=True, desc=\"CARE zip\") as pbar:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    print(\"‚úì Download complete!\")\n",
    "else:\n",
    "    print(f\"‚úì Found existing CARE zip at {ZIP_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXTRACT DATASET\n",
    "# ============================================================\n",
    "if not (DATA_DIR / \"datasets\").is_dir():\n",
    "    print(\"üìÇ Extracting CARE datasets...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "        z.extractall(DATA_DIR)\n",
    "    print(f\"‚úì Extracted to {DATA_DIR}\")\n",
    "else:\n",
    "    print(\"‚úì CARE datasets already extracted\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset Extraction Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01058a9",
   "metadata": {},
   "source": [
    "## üìä Step 3: Data Loading and Preparation\n",
    "\n",
    "### 3.1 Loading Test Datasets\n",
    "\n",
    "We'll load multiple test splits to ensure comprehensive coverage of sequence similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81786333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD TEST SPLITS\n",
    "# ============================================================\n",
    "print(\"üì• Loading test dataset splits...\\n\")\n",
    "\n",
    "test_file_1 = DATA_DIR / \"datasets/splits/task1/30-50_protein_test.csv\" \n",
    "test_file_2 = DATA_DIR / \"datasets/splits/task1/30_protein_test.csv\" \n",
    "test_file_3 = DATA_DIR / \"datasets/splits/task1/50-70_protein_test.csv\" \n",
    "\n",
    "# Concatenate all test splits\n",
    "test_df = pd.concat([\n",
    "    pd.read_csv(test_file_1),\n",
    "    pd.read_csv(test_file_2),\n",
    "    pd.read_csv(test_file_3)\n",
    "])\n",
    "\n",
    "print(f\"‚úì Loaded test dataframe: {test_df.shape[0]} samples\")\n",
    "print(f\"\\nüìã First few rows:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Define sequence column\n",
    "seq_col = \"Sequence\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cec527",
   "metadata": {},
   "source": [
    "### 3.2 Preparing EC Labels\n",
    "\n",
    "We'll filter the dataset to include only the top EC classes and create label mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE LABEL PARAMETERS\n",
    "# ============================================================\n",
    "LABEL_COL = \"EC1\"  # Top level of EC hierarchy\n",
    "TOP_N = 10  # Number of top classes to keep\n",
    "\n",
    "# ============================================================\n",
    "# SELECT TOP CLASSES\n",
    "# ============================================================\n",
    "label_counts = test_df[LABEL_COL].value_counts()\n",
    "valid_labels = label_counts.nlargest(TOP_N).index.tolist()\n",
    "\n",
    "print(f\"üìä Selected top {len(valid_labels)} EC1 classes:\")\n",
    "for i, label in enumerate(valid_labels, 1):\n",
    "    count = label_counts[label]\n",
    "    print(f\"   {i}. EC{label}: {count} samples\")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE LABEL MAPPINGS\n",
    "# ============================================================\n",
    "label_to_id = {l: i for i, l in enumerate(valid_labels)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "# ============================================================\n",
    "# FILTER AND MAP LABELS\n",
    "# ============================================================\n",
    "test_df = test_df[test_df[LABEL_COL].isin(valid_labels)].copy()\n",
    "test_df['label_id'] = test_df[LABEL_COL].map(label_to_id)\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Test samples:    {test_df.shape[0]}\")\n",
    "print(f\"  Number of classes: {len(valid_labels)}\")\n",
    "print(f\"  Sequence column:   '{seq_col}'\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ee2cc",
   "metadata": {},
   "source": [
    "## ü§ñ Step 4: ESM Model Setup\n",
    "\n",
    "### 4.1 Loading Pre-trained ESM2 Model\n",
    "\n",
    "We'll load a frozen ESM2 model to extract protein embeddings. The model is kept in evaluation mode and parameters are frozen since we're only extracting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c793945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE COMPUTE BACKEND\n",
    "# ============================================================\n",
    "def get_backend():\n",
    "    \"\"\"\n",
    "    Detect and return the available compute backend.\n",
    "    Returns: (device, backend_name, n_gpus)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    backend = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    n_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "    return device, backend, n_gpus\n",
    "\n",
    "DEVICE, BACKEND, N_GPUS = get_backend()\n",
    "\n",
    "print(\"üñ•Ô∏è  Compute Configuration:\")\n",
    "print(f\"   Device:  {DEVICE}\")\n",
    "print(f\"   Backend: {BACKEND}\")\n",
    "print(f\"   GPUs:    {N_GPUS}\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD ESM2 MODEL AND TOKENIZER\n",
    "# ============================================================\n",
    "MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"  # 8M parameter model\n",
    "\n",
    "print(f\"\\nüß¨ Loading ESM2 model: {MODEL_NAME}\")\n",
    "print(\"   Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"   ‚úì Tokenizer loaded\")\n",
    "\n",
    "print(\"   Loading model...\")\n",
    "esm_model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "print(\"   ‚úì Model loaded\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURE MODEL FOR EMBEDDING EXTRACTION\n",
    "# ============================================================\n",
    "print(\"\\n‚öôÔ∏è  Configuring model:\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "esm_model.eval()\n",
    "print(\"   ‚úì Set to evaluation mode\")\n",
    "\n",
    "# Convert to half precision for efficiency (CUDA only)\n",
    "if torch.cuda.is_available():\n",
    "    esm_model.half()\n",
    "    print(\"   ‚úì Converted to half precision (float16)\")\n",
    "else:\n",
    "    print(\"   ‚úì Using full precision (CPU mode)\")\n",
    "\n",
    "# Freeze all parameters (no training)\n",
    "for p in esm_model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(\"   ‚úì Frozen all parameters\")\n",
    "\n",
    "print(\"\\n‚úì ESM2 model ready for embedding extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffd8da",
   "metadata": {},
   "source": [
    "### 4.2 Defining Embedding Extraction Functions\n",
    "\n",
    "We'll create utility functions to efficiently extract embeddings in batches, handling both sorting for efficiency and preserving original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EMBEDDING EXTRACTION FUNCTION\n",
    "# ============================================================\n",
    "def get_esm_embeddings(seqs, model, tokenizer, device, max_batch=256):\n",
    "    \"\"\"\n",
    "    Extract ESM embeddings for a list of protein sequences.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seqs : list of str\n",
    "        Protein sequences to embed\n",
    "    model : AutoModel\n",
    "        Pre-loaded ESM model\n",
    "    tokenizer : AutoTokenizer\n",
    "        Pre-loaded ESM tokenizer\n",
    "    device : torch.device\n",
    "        Device to run on (CPU/GPU)\n",
    "    max_batch : int\n",
    "        Maximum batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of embeddings, shape (n_sequences, embedding_dim)\n",
    "    \"\"\"\n",
    "    embs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(seqs), max_batch), desc=\"Extracting embeddings\"):\n",
    "            # Get batch of sequences\n",
    "            batch = seqs[i:i+max_batch]\n",
    "            \n",
    "            # Tokenize sequences\n",
    "            toks = tokenizer(\n",
    "                batch, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                add_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            toks = {k: v.to(device) for k, v in toks.items()}\n",
    "            \n",
    "            # Extract embeddings\n",
    "            out = model(**toks, return_dict=True)\n",
    "            \n",
    "            # Get CLS token embeddings (first token)\n",
    "            cls_emb = out.last_hidden_state[:, 0, :]\n",
    "            \n",
    "            # Convert to float32, detach, move to CPU\n",
    "            embs.append(cls_emb.float().detach().cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_embs = torch.cat(embs, dim=0)\n",
    "    \n",
    "    # Convert to numpy (using tolist as workaround for potential issues)\n",
    "    return np.array(all_embs.tolist())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTION: MAINTAIN ORIGINAL ORDER\n",
    "# ============================================================\n",
    "def embed_sequences_in_original_order(seqs, model, tokenizer, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract embeddings while maintaining original sequence order.\n",
    "    Sequences are sorted by length for efficiency, then unsorted.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seqs : list of str\n",
    "        Protein sequences to embed\n",
    "    model, tokenizer, device : \n",
    "        Model components\n",
    "    **kwargs : \n",
    "        Additional arguments for get_esm_embeddings\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Embeddings in original sequence order\n",
    "    \"\"\"\n",
    "    # Track original indices\n",
    "    indexed = list(enumerate(seqs))\n",
    "    \n",
    "    # Sort by sequence length for efficiency\n",
    "    indexed_sorted = sorted(indexed, key=lambda x: len(x[1]))\n",
    "    sorted_indices, sorted_seqs = zip(*indexed_sorted)\n",
    "    \n",
    "    # Get embeddings in sorted order\n",
    "    sorted_embs = get_esm_embeddings(sorted_seqs, model, tokenizer, device, **kwargs)\n",
    "    \n",
    "    # Unsort to original order\n",
    "    sorted_indices = np.array(sorted_indices)\n",
    "    reverse_idx = np.argsort(sorted_indices)\n",
    "    \n",
    "    return sorted_embs[reverse_idx]\n",
    "\n",
    "print(\"‚úì Embedding extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37492797",
   "metadata": {},
   "source": [
    "## üî¨ Step 5: Extracting Protein Embeddings\n",
    "\n",
    "### 5.1 Generating Test Set Embeddings\n",
    "\n",
    "Now we'll extract embeddings for all protein sequences in our test set. This process captures the latent representations learned by ESM2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXTRACT TEST SET EMBEDDINGS\n",
    "# ============================================================\n",
    "print(\"üîÑ Extracting test embeddings...\\n\")\n",
    "\n",
    "X_test = embed_sequences_in_original_order(\n",
    "    test_df[seq_col].tolist(), \n",
    "    esm_model, \n",
    "    tokenizer, \n",
    "    DEVICE\n",
    ")\n",
    "y_test = test_df['label_id'].values\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Embedding Extraction Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Embeddings shape:    {X_test.shape}\")\n",
    "print(f\"  Number of samples:   {X_test.shape[0]}\")\n",
    "print(f\"  Embedding dimension: {X_test.shape[1]}\")\n",
    "print(f\"  Labels shape:        {y_test.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91912242",
   "metadata": {},
   "source": [
    "## üìâ Step 6: UMAP Dimensionality Reduction\n",
    "\n",
    "### 6.1 Reducing to 2D for Visualization\n",
    "\n",
    "We'll apply **UMAP (Uniform Manifold Approximation and Projection)** to reduce the high-dimensional embeddings (320D) to 2D for visualization while preserving the manifold structure.\n",
    "\n",
    "**UMAP Parameters:**\n",
    "- `n_components=2`: Reduce to 2 dimensions\n",
    "- `n_neighbors=15`: Local neighborhood size\n",
    "- `min_dist=0.1`: Minimum distance between points in low-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# APPLY UMAP DIMENSIONALITY REDUCTION\n",
    "# ============================================================\n",
    "print(\"üìâ Applying UMAP dimensionality reduction...\\n\")\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2, \n",
    "    random_state=42, \n",
    "    n_neighbors=15, \n",
    "    min_dist=0.1\n",
    ")\n",
    "\n",
    "X_test_2d = reducer.fit_transform(X_test)\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "print(\"‚úì UMAP reduction complete!\")\n",
    "print(f\"\\n  Original shape:  {X_test.shape}\")\n",
    "print(f\"  Reduced shape:   {X_test_2d.shape}\")\n",
    "print(f\"  Dimensions reduced: {X_test.shape[1]} ‚Üí {X_test_2d.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f75b79",
   "metadata": {},
   "source": [
    "### 6.2 Visualizing 2D Embeddings\n",
    "\n",
    "Let's visualize the reduced embeddings, colored by EC class, to see how well the model separates different protein functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95227773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE 2D SCATTER PLOT\n",
    "# ============================================================\n",
    "print(\"üé® Creating visualization...\\n\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    X_test_2d[:, 0], \n",
    "    X_test_2d[:, 1], \n",
    "    c=y_test, \n",
    "    cmap='tab10', \n",
    "    alpha=0.7,\n",
    "    s=50,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, label='EC Class ID')\n",
    "plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "plt.title('UMAP 2D Visualization of Protein Embeddings (Colored by EC Class)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization complete!\")\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Points represent protein sequences in 2D latent space\")\n",
    "print(\"   - Colors indicate EC class (enzyme function)\")\n",
    "print(\"   - Clusters suggest the model groups similar functions together\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09f6ce",
   "metadata": {},
   "source": [
    "## üéØ Step 7: Clustering Analysis\n",
    "\n",
    "### 7.1 K-Means Clustering\n",
    "\n",
    "We'll apply K-means clustering to identify groups of similar samples in the latent space and assess how well they align with true EC classes.\n",
    "\n",
    "**Goal:** Determine if samples from the same EC class cluster together in the latent space.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "1. **Mutual Information (MI)**: Measures shared information between clusters and true labels\n",
    "2. **Adjusted Mutual Information (AMI)**: MI adjusted for chance (ranges from -1 to 1, 1 = perfect)\n",
    "3. **Normalized Mutual Information (NMI)**: MI normalized (ranges from 0 to 1, 1 = perfect)\n",
    "\n",
    "For more details, see the [Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/clustering.html#mutual-info-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PERFORM K-MEANS CLUSTERING\n",
    "# ============================================================\n",
    "print(\"üéØ Performing K-means clustering...\\n\")\n",
    "\n",
    "n_clusters = len(np.unique(y_test))\n",
    "print(f\"Number of clusters (matching true classes): {n_clusters}\")\n",
    "\n",
    "# Cluster the UMAP embeddings\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_test_2d)\n",
    "\n",
    "print(f\"‚úì Clustering complete!\")\n",
    "\n",
    "# ============================================================\n",
    "# CALCULATE MUTUAL INFORMATION SCORES\n",
    "# ============================================================\n",
    "print(\"\\nüìä Computing mutual information scores...\")\n",
    "\n",
    "mi_score = mutual_info_score(y_test, cluster_labels)\n",
    "ami_score = adjusted_mutual_info_score(y_test, cluster_labels)\n",
    "nmi_score = normalized_mutual_info_score(y_test, cluster_labels)\n",
    "\n",
    "# ============================================================\n",
    "# DISPLAY RESULTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Mutual Information Scores for UMAP Clustering:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Mutual Information (MI):              {mi_score:.4f}\")\n",
    "print(f\"  Adjusted Mutual Information (AMI):    {ami_score:.4f}\")\n",
    "print(f\"  Normalized Mutual Information (NMI):  {nmi_score:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìö Score Interpretation:\")\n",
    "print(\"   - MI ranges from 0 to log(n_clusters)\")\n",
    "print(\"   - AMI ranges from -1 to 1 (adjusted for chance, 1 = perfect)\")\n",
    "print(\"   - NMI ranges from 0 to 1 (normalized, 1 = perfect)\")\n",
    "\n",
    "print(f\"\\nüí° AMI Score Assessment: {ami_score:.4f} - \", end=\"\")\n",
    "if ami_score > 0.7:\n",
    "    print(\"Excellent clustering! üéâ\")\n",
    "elif ami_score > 0.5:\n",
    "    print(\"Good clustering ‚úì\")\n",
    "elif ami_score > 0.3:\n",
    "    print(\"Moderate clustering\")\n",
    "else:\n",
    "    print(\"Poor clustering - optimization recommended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208d4ad",
   "metadata": {},
   "source": [
    "### 7.2 Comparing True Labels vs Predicted Clusters\n",
    "\n",
    "Let's visualize both the true EC classes and the K-means clusters side-by-side to assess clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb113f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE COMPARISON VISUALIZATION\n",
    "# ============================================================\n",
    "print(\"üé® Creating side-by-side comparison...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# ============================================================\n",
    "# PLOT 1: TRUE EC CLASSES\n",
    "# ============================================================\n",
    "scatter1 = axes[0].scatter(\n",
    "    X_test_2d[:, 0], X_test_2d[:, 1], \n",
    "    c=y_test, \n",
    "    cmap='tab10', \n",
    "    alpha=0.7,\n",
    "    s=50,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "axes[0].set_xlabel('UMAP Component 1', fontsize=12)\n",
    "axes[0].set_ylabel('UMAP Component 2', fontsize=12)\n",
    "axes[0].set_title(f'True EC Classes (n={n_clusters})', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='True EC Class')\n",
    "\n",
    "# ============================================================\n",
    "# PLOT 2: K-MEANS CLUSTERS\n",
    "# ============================================================\n",
    "scatter2 = axes[1].scatter(\n",
    "    X_test_2d[:, 0], X_test_2d[:, 1], \n",
    "    c=cluster_labels, \n",
    "    cmap='tab10', \n",
    "    alpha=0.7,\n",
    "    s=50,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "axes[1].set_xlabel('UMAP Component 1', fontsize=12)\n",
    "axes[1].set_ylabel('UMAP Component 2', fontsize=12)\n",
    "axes[1].set_title(f'K-Means Clusters (AMI={ami_score:.3f})', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster ID')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Comparison visualization complete!\")\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   - Left: Ground truth EC classes\")\n",
    "print(\"   - Right: Unsupervised K-means clusters\")\n",
    "print(\"   - Higher AMI = better agreement between plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee54a3",
   "metadata": {},
   "source": [
    "## ‚ö° Step 8: Hyperparameter Optimization with Optuna\n",
    "\n",
    "### 8.1 Motivation for Optimization\n",
    "\n",
    "The clustering quality from our initial UMAP parameters may not be optimal. We can improve the **Adjusted Mutual Information (AMI)** score by systematically searching for better UMAP hyperparameters.\n",
    "\n",
    "**Parameters to Optimize:**\n",
    "- `n_neighbors`: Controls local vs global structure preservation (5-50)\n",
    "- `min_dist`: Minimum distance between points in embedding (0.0-0.99)\n",
    "- `metric`: Distance metric ('euclidean', 'manhattan', 'cosine')\n",
    "\n",
    "We'll use [**Optuna**](https://optuna.org/) - a hyperparameter optimization framework that efficiently searches the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a29a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEFINE OPTUNA OBJECTIVE FUNCTION\n",
    "# ============================================================\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna optimization.\n",
    "    \n",
    "    For each trial:\n",
    "    1. Suggest UMAP hyperparameters\n",
    "    2. Apply UMAP transformation\n",
    "    3. Perform K-means clustering\n",
    "    4. Calculate AMI score (metric to maximize)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        AMI score for this set of hyperparameters\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 5, 50)\n",
    "    min_dist = trial.suggest_float('min_dist', 0.0, 0.99)\n",
    "    metric = trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'cosine'])\n",
    "    \n",
    "    # Apply UMAP with suggested hyperparameters\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=metric,\n",
    "        random_state=42\n",
    "    )\n",
    "    X_2d = reducer.fit_transform(X_test)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    n_clusters = len(np.unique(y_test))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_2d)\n",
    "    \n",
    "    # Calculate AMI score (our metric to maximize)\n",
    "    ami = adjusted_mutual_info_score(y_test, cluster_labels)\n",
    "    \n",
    "    return ami\n",
    "\n",
    "print(\"‚úì Optimization objective defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec4068",
   "metadata": {},
   "source": [
    "### 8.2 Running Optimization\n",
    "\n",
    "We'll run 50 trials to find the best combination of UMAP hyperparameters that maximize the AMI score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE AND RUN OPTUNA STUDY\n",
    "# ============================================================\n",
    "print(\"‚ö° Starting Optuna hyperparameter optimization...\")\n",
    "print(\"üîç Running 50 trials to find best UMAP parameters...\")\n",
    "print(\"‚è≥ This may take a few minutes...\\n\")\n",
    "\n",
    "# Suppress Optuna's verbose logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Create study (maximize AMI score)\n",
    "study = optuna.create_study(direction='maximize', study_name='umap_optimization')\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# ============================================================\n",
    "# DISPLAY RESULTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTUNA OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"\\n‚úì Best trial found:\")\n",
    "print(f\"   AMI Score: {study.best_value:.4f}\")\n",
    "print(f\"\\nüéØ Best hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "improvement = study.best_value - ami_score\n",
    "improvement_pct = (improvement / ami_score * 100) if ami_score > 0 else 0\n",
    "\n",
    "print(f\"\\nüìà Improvement over initial parameters:\")\n",
    "print(f\"   Initial AMI: {ami_score:.4f}\")\n",
    "print(f\"   Best AMI:    {study.best_value:.4f}\")\n",
    "print(f\"   Gain:        +{improvement:.4f} ({improvement_pct:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764391c",
   "metadata": {},
   "source": [
    "### 8.3 üìä Apply Best Parameters\n",
    "\n",
    "Now we'll use the optimized hyperparameters to create an improved UMAP embedding and compare it with our initial results.\n",
    "\n",
    "**What to expect:**\n",
    "- **Better cluster separation**: Optimized parameters should create more distinct clusters\n",
    "- **Higher AMI score**: Better alignment between clusters and true EC labels\n",
    "- **Visual comparison**: Side-by-side plots showing the improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5473786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE OPTIMIZED UMAP EMBEDDING\n",
    "# ============================================================\n",
    "print(\"üîß Creating UMAP with optimized parameters...\")\n",
    "\n",
    "# Initialize UMAP with best parameters\n",
    "reducer_optimized = umap.UMAP(\n",
    "    n_neighbors=study.best_params['n_neighbors'],\n",
    "    min_dist=study.best_params['min_dist'],\n",
    "    n_components=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "umap_optimized = reducer_optimized.fit_transform(X_test)\n",
    "\n",
    "# Cluster with optimized embedding\n",
    "kmeans_optimized = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels_optimized = kmeans_optimized.fit_predict(umap_optimized)\n",
    "\n",
    "# Calculate new AMI score\n",
    "ami_optimized = adjusted_mutual_info_score(y_test, labels_optimized)\n",
    "\n",
    "print(f\"‚úì Optimized embedding created successfully!\")\n",
    "print(f\"   New AMI Score: {ami_optimized:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZE OPTIMIZED CLUSTERING\n",
    "# ============================================================\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Original clustering\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter1 = plt.scatter(X_test_2d[:, 0], X_test_2d[:, 1], \n",
    "                       c=y_test, cmap='tab10', alpha=0.6, s=10)\n",
    "plt.title(f'Original Parameters\\nAMI: {ami_score:.4f}', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.colorbar(scatter1, label='Cluster')\n",
    "\n",
    "# Optimized clustering\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter2 = plt.scatter(umap_optimized[:, 0], umap_optimized[:, 1], \n",
    "                       c=labels_optimized, cmap='tab10', alpha=0.6, s=10)\n",
    "plt.title(f'Optimized Parameters\\nAMI: {ami_optimized:.4f}', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.colorbar(scatter2, label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Comparison Summary:\")\n",
    "print(f\"   Improvement: +{ami_optimized - ami_score:.4f} AMI points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e576c",
   "metadata": {},
   "source": [
    "### 8.4 üìà Optimization History Visualization\n",
    "\n",
    "Let's visualize how Optuna explored the hyperparameter space and improved the AMI score over successive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6576c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZE OPTIMIZATION HISTORY\n",
    "# ============================================================\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# Plot 1: Optimization history (AMI score over trials)\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig1.update_layout(title=\"Optimization History: AMI Score vs. Trial Number\")\n",
    "fig1.show()\n",
    "\n",
    "# Plot 2: Parameter importances (which parameters matter most)\n",
    "fig2 = plot_param_importances(study)\n",
    "fig2.update_layout(title=\"Hyperparameter Importance for AMI Score\")\n",
    "fig2.show()\n",
    "\n",
    "print(\"\\nüí° Insights from optimization:\")\n",
    "print(\"   - The optimization history shows how quickly Optuna found good parameters\")\n",
    "print(\"   - Parameter importance reveals which hyperparameters most affect clustering quality\")\n",
    "print(\"   - This helps us understand the latent space structure better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b7e46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Step 9: Layer-wise Hidden State Analysis\n",
    "\n",
    "So far we've analyzed the final embeddings from ESM. But what happens **inside** the model? How do protein representations evolve as they pass through different layers?\n",
    "\n",
    "### Why analyze hidden states?\n",
    "\n",
    "**Understanding layer-wise evolution helps us:**\n",
    "- **Visualize feature learning**: See how raw sequences become meaningful representations\n",
    "- **Identify critical layers**: Find which layers capture the most relevant information\n",
    "- **Debug model behavior**: Understand what patterns the model learns at each depth\n",
    "- **Optimize extraction**: Choose the best layer for downstream tasks\n",
    "\n",
    "**What we'll explore:**\n",
    "1. Extract hidden states from all 33 ESM layers\n",
    "2. Analyze how representations change across layers\n",
    "3. Visualize layer-wise feature evolution\n",
    "4. Compare early vs. late layer representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900cca8",
   "metadata": {},
   "source": [
    "### 9.1 üî¨ Extract Sample Sequence Hidden States\n",
    "\n",
    "First, let's extract hidden states from all layers for a sample protein sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea796f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GET SAMPLE SEQUENCE AND TOKENIZE\n",
    "# ============================================================\n",
    "print(\"üìã Extracting sample protein sequence...\")\n",
    "\n",
    "# Get a sample protein sequence from the test set\n",
    "sample_sequence = test_df[seq_col].iloc[0]\n",
    "print(f\"Sample sequence: {sample_sequence[:50]}...\")  # Show first 50 characters\n",
    "print(f\"Length: {len(sample_sequence)} amino acids\")\n",
    "\n",
    "# Tokenize the sequence\n",
    "print(\"\\nüî§ Tokenizing sequence...\")\n",
    "inputs = tokenizer(sample_sequence, return_tensors=\"pt\", padding=True, truncation=True, add_special_tokens=True)\n",
    "inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"‚úì Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"  (batch_size=1, sequence_length={inputs['input_ids'].shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXTRACT HIDDEN STATES FROM ALL LAYERS\n",
    "# ============================================================\n",
    "print(\"üß† Extracting hidden states from all ESM layers...\")\n",
    "\n",
    "# Extract hidden states from all layers\n",
    "with torch.no_grad():\n",
    "    outputs = esm_model(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states  # Tuple of tensors (num_layers, batch_size, seq_len, hidden_dim)\n",
    "    \n",
    "print(f\"‚úì Number of layers: {len(hidden_states)}\")\n",
    "print(f\"  Hidden state shape per layer: {hidden_states[0].shape}\")\n",
    "print(f\"  (batch_size, sequence_length, hidden_dimension)\")\n",
    "\n",
    "# ============================================================\n",
    "# EXTRACT CLS TOKEN REPRESENTATIONS ACROSS LAYERS\n",
    "# ============================================================\n",
    "print(\"\\nüìä Extracting CLS token representations...\")\n",
    "\n",
    "# Get the CLS token (position 0) representation across all layers\n",
    "cls_representations = torch.stack([h[:, 0, :] for h in hidden_states])  # (num_layers, batch_size, hidden_dim)\n",
    "cls_representations = cls_representations.squeeze(1).float().cpu()  # (num_layers, hidden_dim)\n",
    "\n",
    "# Convert to numpy using tolist workaround\n",
    "cls_representations = np.array(cls_representations.tolist())\n",
    "\n",
    "print(f\"‚úì CLS representations shape: {cls_representations.shape}\")\n",
    "print(f\"  ({len(hidden_states)} layers √ó {cls_representations.shape[1]} dimensions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790870c5",
   "metadata": {},
   "source": [
    "### 9.2 üìè Analyze Representation Norms Across Layers\n",
    "\n",
    "Let's visualize how the magnitude (norm) of representations changes as information flows through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1162df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPUTE AND VISUALIZE LAYER NORMS\n",
    "# ============================================================\n",
    "print(\"üìä Computing representation norms...\")\n",
    "\n",
    "# Compute the L2 norm for each layer's CLS representation\n",
    "layer_norms = np.linalg.norm(cls_representations, axis=1)\n",
    "\n",
    "print(f\"‚úì Norms computed for {len(layer_norms)} layers\")\n",
    "\n",
    "# Visualize norm evolution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Line plot of norms\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(layer_norms)), layer_norms, marker='o', linewidth=2, markersize=4)\n",
    "plt.xlabel('Layer Index', fontsize=11)\n",
    "plt.ylabel('L2 Norm', fontsize=11)\n",
    "plt.title('CLS Token Representation Norm Across Layers', fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Bar plot highlighting early vs late layers\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['skyblue' if i < len(layer_norms)//2 else 'salmon' for i in range(len(layer_norms))]\n",
    "plt.bar(range(len(layer_norms)), layer_norms, color=colors, alpha=0.7)\n",
    "plt.xlabel('Layer Index', fontsize=11)\n",
    "plt.ylabel('L2 Norm', fontsize=11)\n",
    "plt.title('Early Layers (Blue) vs Late Layers (Red)', fontweight='bold')\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"   Early layers (0-16): Avg norm = {np.mean(layer_norms[:len(layer_norms)//2]):.2f}\")\n",
    "print(f\"   Late layers (17-33): Avg norm = {np.mean(layer_norms[len(layer_norms)//2:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b484be",
   "metadata": {},
   "source": [
    "### 9.3 üîÑ Layer-to-Layer Similarity Analysis\n",
    "\n",
    "How much do representations change between consecutive layers? Let's compute cosine similarity to measure layer-to-layer changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPUTE COSINE SIMILARITY BETWEEN CONSECUTIVE LAYERS\n",
    "# ============================================================\n",
    "print(\"üîç Computing layer-to-layer cosine similarity...\")\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between consecutive layers\n",
    "layer_similarities = []\n",
    "for i in range(len(cls_representations) - 1):\n",
    "    sim = cosine_similarity(cls_representations[i:i+1], cls_representations[i+1:i+2])[0, 0]\n",
    "    layer_similarities.append(sim)\n",
    "\n",
    "print(f\"‚úì Computed {len(layer_similarities)} layer-to-layer similarities\")\n",
    "\n",
    "# Visualize similarity changes\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(1, len(layer_similarities) + 1), layer_similarities, \n",
    "         marker='o', linewidth=2, markersize=5, color='purple')\n",
    "plt.axhline(y=np.mean(layer_similarities), color='red', linestyle='--', \n",
    "            label=f'Mean similarity: {np.mean(layer_similarities):.4f}', linewidth=2)\n",
    "plt.xlabel('Layer Transition (i ‚Üí i+1)', fontsize=11)\n",
    "plt.ylabel('Cosine Similarity', fontsize=11)\n",
    "plt.title('How Much Do Representations Change Between Layers?', fontweight='bold', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"   High similarity (>0.95): Layers are similar ‚Üí incremental refinement\")\n",
    "print(f\"   Low similarity (<0.90): Layers differ more ‚Üí major transformations\")\n",
    "print(f\"   Average similarity: {np.mean(layer_similarities):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718604c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Step 10: Wrap-Up & Key Takeaways\n",
    "\n",
    "Congratulations! You've completed a comprehensive exploration of latent space analysis for protein embeddings. \n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "**1. üìä Dimensionality Reduction**\n",
    "- Used UMAP to visualize high-dimensional protein embeddings in 2D\n",
    "- Understood the importance of hyperparameters (`n_neighbors`, `min_dist`)\n",
    "- Learned how to interpret UMAP plots for protein clustering\n",
    "\n",
    "**2. üî¨ Clustering Analysis**\n",
    "- Applied K-means clustering to discover functional groups\n",
    "- Used Adjusted Mutual Information (AMI) to evaluate cluster quality\n",
    "- Compared clustering results with known EC classifications\n",
    "\n",
    "**3. ‚ö° Hyperparameter Optimization**\n",
    "- Leveraged Optuna for automated hyperparameter search\n",
    "- Optimized UMAP parameters to improve clustering quality\n",
    "- Visualized the optimization process and parameter importance\n",
    "\n",
    "**4. üß† Hidden State Analysis**\n",
    "- Extracted and analyzed layer-wise representations from ESM\n",
    "- Tracked how representations evolve through the network\n",
    "- Measured layer-to-layer similarity to understand feature learning\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "üí° **Latent spaces reveal biological structure**: Even without labels, embeddings cluster proteins by function\n",
    "\n",
    "üí° **Hyperparameters matter**: Small changes in UMAP settings can significantly impact visualization quality\n",
    "\n",
    "üí° **Deep learning is hierarchical**: Early layers learn basic patterns, late layers capture complex functional relationships\n",
    "\n",
    "üí° **Optimization is powerful**: Automated search can find better parameters than manual tuning\n",
    "\n",
    "### Next Steps & Extensions\n",
    "\n",
    "üîç **Try these experiments:**\n",
    "1. **Different distance metrics**: Try other distance functions in UMAP (e.g., cosine, manhattan)\n",
    "2. **Alternative clustering**: Experiment with DBSCAN or hierarchical clustering\n",
    "3. **Layer selection**: Extract embeddings from specific layers instead of mean pooling\n",
    "4. **Multi-modal analysis**: Combine sequence embeddings with other protein features\n",
    "5. **Biological validation**: Test if clusters correspond to substrate specificity or other properties\n",
    "\n",
    "### Reproducibility Notes\n",
    "\n",
    "```python\n",
    "# Key parameters used in this notebook:\n",
    "RANDOM_SEED = 42\n",
    "UMAP_PARAMS = {'n_neighbors': 15, 'min_dist': 0.1}\n",
    "KMEANS_CLUSTERS = 10\n",
    "OPTUNA_TRIALS = 50\n",
    "ESM_MODEL = 'facebook/esm2_t33_650M_UR50D'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Great work!** You now have the tools to explore latent spaces in any protein embedding model. These techniques are widely applicable across bioinformatics, from drug discovery to protein engineering. Keep experimenting! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
